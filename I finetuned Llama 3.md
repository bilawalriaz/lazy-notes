I finetuned Llama 3.2 3B to clean up raw transcripts and perform analysis on it including categorising, tagging, basic NLP extraction etc. I trained it to output a JSON object which i then parse to create a HTML card.

# Backstory
I wrote a simple app that watches a folder for audio files and transcribes them using Whisper (all platforms) or Parakeet (Mac). I would then send the transcripts to OpenRouter to clean up, but i thought seeing as though the transcription is being done locally, can i do the cleaning locally too? I tested it out with llama3.2-3b with a robust prompt and the results were decent albeit incomplete (not that many entities extracted, issues with dates etc).
I decided to see how i could improve this using SFT.



# Dataset
I took 13 random voice memos and transcribed them using my app. I used those transcripts to generate synthetic examples using 5-8 real transcripts per prompt. I generated over 40k synthetic examples, each of which i sent to a SOTA LLM (Kimi K2) to be processed into a gold standard JSON output - the kind of output i want my fine tune to produce.


# Evals
I took 50 random samples from my validation set and processed the raw transcripts using the un-finetuned llama3.2-3b as well as the fine tune. These were then compared with each other as well as the already present gold standard JSON example generated by a SOTA model.